{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img \n",
    "    style=\"position: absolute; \n",
    "           left: 60%; \n",
    "           top: 0; /* Added to ensure proper positioning */\n",
    "           height: 500px; \n",
    "           width: 40%; /* Maintain the original width */\n",
    "           object-fit: cover; /* Adjust if necessary */\n",
    "           clip-path: inset(0px 50px 0px 50px round 10px);\" \n",
    "    src= \"../.github/public_html/fig/Major crack Width development.png\"\n",
    "/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<h1 style=\"width: 60%; color: #EC6842; font-size: 55pt;\">\n",
    "    <Strong>\n",
    "        FEA Study\n",
    "    </Strong>\n",
    "</h1>\n",
    "\n",
    "<h2 id=\"Background\"><B>\n",
    "    Rationale for the project<a class=\"anchor-link\" href=\"#Background\">&#182;</a>\n",
    "    </B>\n",
    "</h2>\n",
    "<p style=\"text-align: justify; width: 60%; font-weight: normal;\">\n",
    "     This studies case study is the building will also evaluate the capabilities of FEA for the assessment of unreinforced masonry structures undergoing subsidence. For this measure understanding and analysing the capabilities of a finite element analysis are a important part in the validation of the model and the presenting of results. This notebook illustrates some of the different tools built for the analysis of such models and its implementations in different analysis types. Due to the limitations of the analysis output characteristics from DIANA FEA not all components of these analysis are able to have been done algorithmically therefore, it is reccomended to read this modules README.MD notes in order to ensure the different requirements for these analysis are well implemented in your models. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> X | Imports</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bricks.fea.analysis import analyse_models, setup_analysis\n",
    "from bricks.fea.analysis.processing.tabulated import process_tb\n",
    "from bricks.fea.analysis.main import *\n",
    "from bricks.fea.analysis.processing.main import *\n",
    "from bricks.fea.analysis.processing.utils import *\n",
    "from bricks.fea.analysis.plots import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 2 | Process runs</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 2.1 | Run individual analysis</strong>\n",
    "\n",
    "The three main individual plots for the relative displacement, crack width progression and damage level progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Crack 0': {'length': 5062.054006033519, 'average_width': 7.876843971631206, 'component': 0, 'elements': [34.0, 35.0, 71.0, 88.0, 110.0, 146.0, 184.0, 211.0, 248.0, 259.0, 260.0, 273.0, 276.0, 293.0, 318.0, 335.0, 336.0, 363.0, 364.0, 367.0, 420.0, 552.0, 569.0, 607.0, 608.0, 609.0, 729.0, 731.0, 736.0, 750.0, 751.0, 794.0, 815.0, 850.0, 877.0, 887.0, 904.0, 966.0]}}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ Manual Analysis ----------------------------- #\n",
    "analysis_info_TSRCM = {\n",
    "    'Mutual': {\n",
    "        'Node Nr': [[22, 23], [22, 23]],\n",
    "        'Reference': [['TDtY', 'TDtY'], ['TDtY', 'TDtX']]\n",
    "    },\n",
    "    'Crack width': {\n",
    "        'EOI': [[177,178,179,435],\n",
    "                [35, 166, 203, 387, 523, 684, 723, 867],\n",
    "                [9, 206, 263, 612]],\n",
    "    },\n",
    "    'Damage level': { \n",
    "        'parameters': {'auto': True,\n",
    "                       'mesh': 200         \n",
    "    }\n",
    "}}\n",
    "\n",
    "plot_settings_TSRCM = {\n",
    "    'Mutual': {\n",
    "        'traces': ['${u_{yB}}/{u_{yA}}$','${u_{xB}}/{u_{yA}}$ '],\n",
    "        'labels': ['Displacement A [mm]', 'Displacement B [mm]'],\n",
    "        'titles': 'Relative displacements at locations of interest',\n",
    "        'scientific': False\n",
    "    },\n",
    "    'Crack width': {\n",
    "        'traces': ['Crack 1','Crack 5','Crack 9'],\n",
    "        'labels': ['Load factor $\\lambda$', 'Crack Width $c_w$ [mm]'],\n",
    "        'titles': 'Major crack Width development',\n",
    "        'scientific': True\n",
    "    },\n",
    "    'Damage level': {\n",
    "        'traces': ['$\\psi$ TSCM'],\n",
    "        'labels': ['Load factor $\\lambda$', 'Damage Parameter $\\psi$'],\n",
    "        'titles': 'Damage level progression',\n",
    "        'scientific': True\n",
    "    }\n",
    "}\n",
    "\n",
    "merge_info = {\n",
    "    'Titles': ['Force norm', 'Displacement norm'],\n",
    "    'x_label': 'Load factor $\\lambda$',\n",
    "    'y_label': 'Disp \\& Force Norm $|\\Delta_f| |\\Delta_u|$',\n",
    "    'title': 'Combined Force and Displacement Norms'\n",
    "}\n",
    "\n",
    "dir = r'C:\\Users\\javie\\OneDrive - Delft University of Technology\\Year 2\\Q3 & Q4\\CIEM0500 - MS Thesis Project\\!content\\Experimentation\\Modelling\\Models\\Continuum\\Standard\\W2O - TS'\n",
    "analyse_models(dir,analysis_info_TSRCM, plot_settings_TSRCM, merge = merge_info)\n",
    "\n",
    "\n",
    "# ---------------------------- Automatic analysis ---------------------------- #\n",
    "analysis_info_mesh = {\n",
    "    'Mutual': {\n",
    "        'Node Nr': [[22, 23], [22, 23]],\n",
    "        'Reference': [['TDtY', 'TDtY'], ['TDtY', 'TDtX']]\n",
    "    },\n",
    "    'Damage level': { \n",
    "        'parameters': {'auto': True,\n",
    "                       'mesh': 200         \n",
    "    }}\n",
    "}\n",
    "\n",
    "plot_settings_mesh = {\n",
    "    'Mutual': {\n",
    "        'traces': ['${u_{yB}}/{u_{yA}}$','${u_{xB}}/{u_{yA}}$'],\n",
    "        'labels': ['Displacement A [mm]', 'Displacement B [mm]'],\n",
    "        'titles': 'Relative displacements at locations of interest',\n",
    "        'scientific': False\n",
    "    },\n",
    "    'Damage level': {\n",
    "        'traces': ['$\\psi$'],\n",
    "        'labels': ['Load factor $\\lambda$', 'Damage Parameter $\\psi$'],\n",
    "        'titles': 'Damage level progression',\n",
    "        'scientific': True\n",
    "    }\n",
    "}\n",
    "\n",
    "merge_info = {\n",
    "    'Titles': ['Force norm', 'Displacement norm'],\n",
    "    'x_label': 'Load factor $\\lambda$',\n",
    "    'y_label': 'Disp \\& Force Norm $|\\Delta_f| |\\Delta_u|$',\n",
    "    'title': 'Combined Force and Displacement Norms'\n",
    "}\n",
    "\n",
    "dir = r'C:\\Users\\javie\\OneDrive - Delft University of Technology\\Year 2\\Q3 & Q4\\CIEM0500 - MS Thesis Project\\!content\\Experimentation\\Modelling\\Models\\Continuum\\Standard\\W2I - TS'\n",
    "rr = analyse_models(dir,analysis_info_mesh, plot_settings_mesh, merge = merge_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 2.2 | Run compared analysis</strong>\n",
    "\n",
    "The three main individual plots for the relative displacement, crack width progression and damage level progression between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- TSRCM ------------------------------ #\n",
    "combined_info_TSRCM = {\n",
    "    'Mutual': {\n",
    "        'Node Nr': [[22, 23], [22, 23]],\n",
    "        'Reference': [['TDtY', 'TDtY'], ['TDtY', 'TDtX']]\n",
    "    },\n",
    "    'Damage level': { \n",
    "        'parameters': {'auto': True,\n",
    "                       'mesh': 200         \n",
    "    }\n",
    "}}\n",
    "\n",
    "combined_settings_TSRCM = {\n",
    "    'Mutual': {\n",
    "        'traces': ['${u_{yB}}/{u_{yA}}$ TSCM','${u_{xB}}/{u_{yA}}$ TSCM'],\n",
    "        'labels': ['Displacement A [mm]', 'Displacement B [mm]'],\n",
    "        'titles': 'Relative displacements at locations of interest',\n",
    "        'scientific': False\n",
    "    },\n",
    "    'Damage level': {\n",
    "        'traces': ['${\\psi_{TSCM}}$ 200mm'],\n",
    "        'labels': ['Load factor $\\lambda$', 'Damage Parameter $\\psi$'],\n",
    "        'titles': 'Damage level progression',\n",
    "        'scientific': True\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "data_TSCM = {}\n",
    "data_TSCM['name'] = 'TSCM-O',\n",
    "data_TSCM['analysis_info'] = combined_info_TSRCM\n",
    "data_TSCM['plot_settings'] = combined_settings_TSRCM\n",
    "data_TSCM['dir'] = r'C:\\Users\\javie\\OneDrive - Delft University of Technology\\Year 2\\Q3 & Q4\\CIEM0500 - MS Thesis Project\\!content\\Experimentation\\Modelling\\Models\\Continuum\\Standard\\W2O - TS\\2DW2O_-_TS_NLA.tb'\n",
    "\n",
    "combined_info_EMMS = {\n",
    "    'Mutual': {\n",
    "        'Node Nr': [[22, 23], [22, 23]],\n",
    "        'Reference': [['TDtY', 'TDtY'], ['TDtY', 'TDtX']]\n",
    "    },\n",
    "    'Damage level': { \n",
    "            'parameters': {'auto': True,\n",
    "                        'mesh': 200},\n",
    "                         }}\n",
    "\n",
    "combined_settings_EMMS = {\n",
    "    'Mutual': {\n",
    "        'traces': ['${u_{yB}}/{u_{yA}}$ EMM','${u_{xB}}/{u_{yA}}$ EMM'],\n",
    "        'labels': ['Displacement A [mm]', 'Displacement B [mm]'],\n",
    "        'titles': 'Relative displacements at locations of interest',\n",
    "        'scientific': False\n",
    "    },\n",
    "    'Damage level': {\n",
    "        'traces': ['${\\psi_{EMM}}$'],\n",
    "        'labels': ['Load factor $\\lambda$', 'Damage Parameter $\\psi$'],\n",
    "        'titles': 'Damage level progression',\n",
    "        'scientific': True\n",
    "    }\n",
    "}\n",
    "\n",
    "data_EMMO = {}\n",
    "data_EMMO['name'] = 'EMM - O'\n",
    "data_EMMO['analysis_info'] = combined_info_EMMS\n",
    "data_EMMO['plot_settings'] = combined_settings_EMMS\n",
    "data_EMMO['dir'] = r'C:\\Users\\javie\\OneDrive - Delft University of Technology\\Year 2\\Q3 & Q4\\CIEM0500 - MS Thesis Project\\!content\\Experimentation\\Modelling\\Models\\EMM\\W2O - EMS\\2DW2O_-_EMS_NLA.tb'\n",
    "\n",
    "plot_data_list = [data_TSCM, data_EMMO]\n",
    "cfigs = compare_models(plot_data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 2.3 | Script to take screenshots and videos of models</strong>\n",
    "\n",
    "Creates a new directory in the models directory where it stores the contour plots for all the specified result components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r''\n",
    "config = {\n",
    "    'results': [\n",
    "        {\n",
    "            'component': 'TDtY',\n",
    "            'result': 'Displacements',\n",
    "            'type': 'Node',\n",
    "            'limits': [-35, -28, -24, -20, -16, -12, -8, -4, 0]\n",
    "        },\n",
    "        {\n",
    "            'component': 'E1',\n",
    "            'result': 'Total Strains',\n",
    "            'type': 'Element',\n",
    "            'location': 'mappedintpnt',\n",
    "            'limits': [-0.004, -0.002, 0, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.02, 0.08]\n",
    "        },\n",
    "        {\n",
    "            'component': 'S1',\n",
    "            'result': 'Cauchy Total Stresses',\n",
    "            'type': 'Element',\n",
    "            'location': 'mappedintpnt',\n",
    "            'limits': [-3.5, -2, -1, -0.05, -0.01, 0, 0.01, 0.05, 1, 3, 61]\n",
    "        },\n",
    "        {\n",
    "            'component': 'Ecw1',\n",
    "            'result': 'Crack-widths',\n",
    "            'type': 'Element',\n",
    "            'location': 'mappedintpnt',\n",
    "            'limits': [0, 1, 2, 3, 4, 5, 10, 15, 20]\n",
    "        }\n",
    "    ],\n",
    "    'script': {\n",
    "        'analysis': \"NLA\",\n",
    "        'load_cases': ['Building', 'Sub Deformation'],\n",
    "        'load_steps': [30, 720],\n",
    "        'load_factors_init': [0.0330000, 0.00138800],\n",
    "        'snapshots': 6,\n",
    "        'view_settings': {\n",
    "            'view_point': [0, 0, 25.0, 0, 1, 0, 5.2, 3.1, 5.5e-17, 19, 3.25],\n",
    "            'title_font_size': 36,\n",
    "            'legend_font_size': 34,\n",
    "            'annotation_font_size': 28\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "setup_analysis(base_path, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 2.4 | Sensitivity effects on damage study</strong>\n",
    "\n",
    "From all models the following code the influence of the damage and time against mesh factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def plot_fits(ax, x, y, models, labels):\n",
    "    vals = np.linspace(min(x), max(x), 100)\n",
    "    for model, label in zip(models, labels):\n",
    "        params, _ = curve_fit(model, x, y)\n",
    "        ax.plot(vals, model(vals, *params), label=f\"{label}: ${params[0]:.4f}x^{{{2 if label == 'Quadratic' else 1}}}$\")\n",
    "    ax.scatter(x, y, color='black', label='Observed Data', marker='s', facecolors='none', s=5, linewidths=0.5)\n",
    "    ax.set_xlabel('Mesh Factor')\n",
    "    ax.legend()\n",
    "\n",
    "observed_values = np.array([132, 156, 0, 0, 528, 707, 161, 468, 1410, 2136, 1300, 1715]) - 0\n",
    "mesh_factor = [1, 1, 1, 1, 2, 2, 2, 2, 4, 4, 4, 4]\n",
    "percentage_diff_damage = np.array([42, 17, 17, 33, 25, 0, 13, 17, 17, 37, 0, 25])\n",
    "mesh_factor_damage = [1, 2, 4] * 4\n",
    "\n",
    "models = [\n",
    "    lambda x, a, b: a * x + b,\n",
    "    lambda x, a, b, c: a * x**2 + b * x + c,\n",
    "    lambda x, a, b: b * np.power(x, a)\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 2.5))\n",
    "\n",
    "plot_fits(axs[0], mesh_factor_damage, percentage_diff_damage, models, ['Linear', 'Quadratic', 'Power Law'])\n",
    "axs[0].set_ylabel(r'DP percentage change $\\Delta \\% \\Psi$')\n",
    "\n",
    "plot_fits(axs[1], mesh_factor, observed_values, models, ['Linear', 'Quadratic', 'Power Law'])\n",
    "axs[1].set_ylabel(r'Analysis time percentage change $\\Delta \\% t$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 3 | Calibration</strong>\n",
    "\n",
    "### <strong> 3.1 | Feature Importance scores</strong>\n",
    "From all the analysis the features are compared against the targets through a random forest to obtain the feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances for df1:\n",
      "Feature 1: 0.492916798644063\n",
      "Feature 2: 0.3143455183565802\n",
      "Feature 3: 0.19273768299935684\n",
      "\n",
      "Feature Importances for df2:\n",
      "Y_init: 0.0\n",
      "X_init: 0.0\n",
      "Feature 1: 0.18772326151437413\n",
      "Feature 2: 0.1527343789029069\n",
      "Feature 3: 0.14914064557695433\n",
      "Feature 4: 0.18972323165041252\n",
      "Feature 5: 0.13111651229497265\n",
      "Feature 6: 0.1895619700603796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from math import pi\n",
    "import pickle\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    df = data[\"monitor_df\"]\n",
    "    targets_expanded = pd.DataFrame(df['Targets'].to_list(), columns=[f'Feature {i+1}' for i in range(len(df['Targets'][0]))])\n",
    "    psi_expanded = pd.DataFrame(df['Psi'].to_list(), columns=['Psi'])\n",
    "    df = pd.concat([df.drop(columns=['Targets', 'Psi', \"Time\", \"Convergence\"]), targets_expanded, psi_expanded], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "file_path_1 = r\"C:\\Users\\javie\\OneDrive - Delft University of Technology\\Year 2\\Q3 & Q4\\CIEM0500 - MS Thesis Project\\!content\\Experimentation\\Calibration\\TSCM Single\\!Results\\Run_2024-08-29_20-07-43\\wall_instance_latest.pkl\"\n",
    "features_1 = [\"Young's Modulus $E_{y}$ [MPa]\", \"Tensile Strength $f_{t}$ [MPa]\", \"Mode I, Fracture Energy $G_{f-I}$ [N/mm]\"]\n",
    "\n",
    "file_path_2 = r\"C:\\Users\\javie\\OneDrive - Delft University of Technology\\Year 2\\Q3 & Q4\\CIEM0500 - MS Thesis Project\\!content\\Experimentation\\Calibration\\EMM Single\\!Results\\Run_2024-09-01_09-41-38\\wall_instance_latest.pkl\"\n",
    "features_2 = [\"Vertical Young's Modulus $E_{y}$ [MPa]\",\"Horizontal Young's Modulus $E_{x}$ [MPa]\",\"Shear Modulus $G$ [MPa]\", \n",
    "              \"Tensile Strength $f_{t}$ [MPa]\", \"Mode I, Fracture Energy $G_{f-I}$ [N/mm]\",\"Mode II, Fracture Energy $G_{f-II}$ [N/mm]\"]\n",
    "\n",
    "df1 = load_and_prepare_data(file_path_1)\n",
    "df2 = load_and_prepare_data(file_path_2)\n",
    "\n",
    "X1 = df1.drop(columns=['Total Loss', 'Psi'])\n",
    "y1 = df1['Total Loss']\n",
    "\n",
    "X2 = df2.drop(columns=['Total Loss', 'Psi'])\n",
    "y2 = df2['Total Loss']\n",
    "\n",
    "model1 = RandomForestRegressor()\n",
    "model2 = RandomForestRegressor()\n",
    "\n",
    "model1.fit(X1, y1)\n",
    "model2.fit(X2, y2)\n",
    "\n",
    "importance1 = model1.feature_importances_\n",
    "importance2 = model2.feature_importances_\n",
    "\n",
    "print(\"Feature Importances for df1:\")\n",
    "for feature, importance in zip(X1.columns, importance1):\n",
    "    print(f'{feature}: {importance}')\n",
    "\n",
    "print(\"\\nFeature Importances for df2:\")\n",
    "for feature, importance in zip(X2.columns, importance2):\n",
    "    print(f'{feature}: {importance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 3.2 | Sensitivity Analysis</strong>\n",
    "\n",
    "#### <strong> 3.2.1 | Plotting the GP solution space</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.priors import GammaPrior\n",
    "from gpytorch.constraints import Interval\n",
    "from scipy.stats import norm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "pickle_file_path = '/content/drive/MyDrive/CALIBRATION/wall_instance_latest_TSCM.pkl'\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    saved_data_tscm = pickle.load(f)\n",
    "\n",
    "material_params = MaterialParameters()\n",
    "confidence_level = 95\n",
    "Ey_bounds = material_params.get_bounds('Emy', confidence_level)\n",
    "fw_bounds = material_params.get_bounds('fw', confidence_level)\n",
    "tensile_strength_bounds = tuple([material_params.tensile_strength(value) for value in fw_bounds])\n",
    "tensile_fracture_energy_bounds = tuple([material_params.tensile_fracture_energy(value, 'mortar') for value in fw_bounds])\n",
    "bounds = [Ey_bounds, tensile_strength_bounds, tensile_fracture_energy_bounds]\n",
    "\n",
    "filtered_df_tscm = saved_data_tscm[\"monitor_df\"][saved_data_tscm[\"monitor_df\"][\"Psi\"].apply(lambda x: x[0] < 7)]\n",
    "\n",
    "X_reco = np.vstack(filtered_df_tscm[\"Targets\"])\n",
    "X_scaler = MinMaxScaler()\n",
    "X_init_single_scaled = X_scaler.fit_transform(X_reco)\n",
    "\n",
    "Y_psi = np.array([i[0] for i in filtered_df_tscm[\"Psi\"].values]).reshape(-1, 1)\n",
    "Y_scaler = MinMaxScaler()\n",
    "Y_psi_scaled = Y_scaler.fit_transform(Y_psi)\n",
    "\n",
    "min_psi_index = np.argmin(Y_psi)\n",
    "optimal_params = X_reco[min_psi_index]\n",
    "\n",
    "likelihood = GaussianLikelihood(noise_constraint=Interval(1e-5, 6.8e-3))  # Regularize\n",
    "kernel = ScaleKernel(\n",
    "    RBFKernel(ard_num_dims=len(bounds), lengthscale_prior=GammaPrior(3.0, 0.5)) +\n",
    "    MaternKernel(nu=2.5, ard_num_dims=len(bounds), lengthscale_prior=GammaPrior(3.0, 0.5))\n",
    ")\n",
    "\n",
    "gp_model_tscm = SingleTaskGP(\n",
    "    train_X=torch.tensor(X_init_single_scaled, dtype=torch.float64),\n",
    "    train_Y=torch.tensor(Y_psi_scaled, dtype=torch.float64),\n",
    "    covar_module=kernel,\n",
    "    likelihood=likelihood\n",
    ")\n",
    "\n",
    "mll = ExactMarginalLogLikelihood(gp_model_tscm.likelihood, gp_model_tscm)\n",
    "fit_gpytorch_mll(mll)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_vars = X_init_single_scaled.shape[1]\n",
    "grid_size = 50  # Adjust based on computational resources\n",
    "\n",
    "# Create linspace values for each parameter based on its bounds\n",
    "linspace_values = [\n",
    "    np.linspace(bounds[i][0], bounds[i][1], grid_size)\n",
    "    for i in range(num_vars)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#                               Plotting the Mean                              #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_vars = 3\n",
    "fig, axes = plt.subplots(num_vars, num_vars, figsize=(12, 10))\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.2)\n",
    "labels = [\"Young's Modulus $E$ [MPa]\", \"Tensile Strength $f_{t}$ [MPa]\", \"Mode I, Fracture Energy $G_{f-I}$ [N/mm]\"]\n",
    "\n",
    "global_min, global_max = float('inf'), float('-inf')\n",
    "\n",
    "for i in range(num_vars):\n",
    "    for j in range(i + 1, num_vars):\n",
    "        X1, X2 = np.meshgrid(linspace_values[i], linspace_values[j])\n",
    "        input_grid = np.zeros((X1.size, num_vars))\n",
    "        input_grid[:, i] = X1.ravel()\n",
    "        input_grid[:, j] = X2.ravel()\n",
    "        for k in range(num_vars):\n",
    "            if k != i and k != j:\n",
    "                input_grid[:, k] = optimal_params[k]\n",
    "\n",
    "        input_grid_normalized = X_scaler.transform(input_grid)\n",
    "        input_grid_normalized = torch.tensor(input_grid_normalized, device=device, dtype=torch.double)\n",
    "        with torch.no_grad():\n",
    "            posterior = gp_model_tscm.posterior(input_grid_normalized)\n",
    "            mean_scaled = posterior.mean.cpu().numpy().reshape(X1.shape)\n",
    "\n",
    "        mean_unscaled = Y_scaler.inverse_transform(mean_scaled.reshape(-1, 1)).reshape(X1.shape)\n",
    "        global_min = min(global_min, mean_unscaled.min())\n",
    "        global_max = max(global_max, mean_unscaled.max())\n",
    "\n",
    "global_min = np.floor(global_min * 100) / 100\n",
    "global_max = np.ceil(global_max * 100) / 100\n",
    "\n",
    "levels = np.linspace(global_min, global_max, 10)  # Change the number of levels as needed\n",
    "levels = np.round(levels, 2)  # Round levels to two decimal places\n",
    "cmap = plt.get_cmap('plasma', len(levels)-1)  # Get a colormap with enough colors for your levels\n",
    "norm = colors.BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
    "\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        ax = axes[i, j]\n",
    "        if i == j:\n",
    "            X1 = linspace_values[i]\n",
    "            input_grid = np.tile(optimal_params, (len(X1), 1))\n",
    "            input_grid[:, i] = X1\n",
    "\n",
    "            input_grid_normalized = X_scaler.transform(input_grid)\n",
    "            input_grid_normalized = torch.tensor(input_grid_normalized, device=device, dtype=torch.double)\n",
    "            with torch.no_grad():\n",
    "                posterior = gp_model_tscm.posterior(input_grid_normalized)\n",
    "                mean_scaled = posterior.mean.cpu().numpy()\n",
    "                std_scaled = posterior.variance.sqrt().cpu().numpy()\n",
    "\n",
    "            mean_unscaled = Y_scaler.inverse_transform(mean_scaled.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "            ax.plot(X1, mean_unscaled, color='black')\n",
    "            ax.set_xlabel(labels[i], labelpad=5)\n",
    "            ax.set_ylabel('Damage Parameter ($\\psi$)', labelpad=5)\n",
    "        else:\n",
    "            X1, X2 = np.meshgrid(linspace_values[i], linspace_values[j])\n",
    "            input_grid = np.zeros((X1.size, num_vars))\n",
    "            input_grid[:, i] = X1.ravel()\n",
    "            input_grid[:, j] = X2.ravel()\n",
    "            for k in range(num_vars):\n",
    "                if k != i and k != j:\n",
    "                    input_grid[:, k] = optimal_params[k]\n",
    "\n",
    "            input_grid_normalized = X_scaler.transform(input_grid)\n",
    "            input_grid_normalized = torch.tensor(input_grid_normalized, device=device, dtype=torch.double)\n",
    "            with torch.no_grad():\n",
    "                posterior = gp_model_tscm.posterior(input_grid_normalized)\n",
    "                mean_scaled = posterior.mean.cpu().numpy().reshape(X1.shape)\n",
    "\n",
    "            mean_unscaled = Y_scaler.inverse_transform(mean_scaled.reshape(-1, 1)).reshape(X1.shape)\n",
    "            cont = ax.contourf(X2, X1, mean_unscaled, levels=levels, cmap=cmap, norm=norm, extend='both')\n",
    "            ax.set_xlabel(labels[j], labelpad=5)\n",
    "            ax.set_ylabel(labels[i], labelpad=5)\n",
    "\n",
    "cbar = fig.colorbar(cont, ax=axes.ravel().tolist(), orientation='vertical', shrink=0.8, aspect=20, extend='both', pad=0.025)\n",
    "cbar.ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "cbar.set_label('Expected Damage Parameter ($\\psi$) \\n **in region close to the optimal solution', rotation=270, labelpad=20)\n",
    "\n",
    "plt.show()\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                             Plotting the variance                            #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming other necessary setups like model, linspace_values, optimal_params, device are defined above.\n",
    "\n",
    "num_vars = 3\n",
    "fig, axes = plt.subplots(num_vars, num_vars, figsize=(12, 10))\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.2)\n",
    "labels = [\"Young's Modulus $E$ [MPa]\", \"Tensile Strength $f_{t}$ [MPa]\", \"Mode I, Fracture Energy $G_{f-I}$ [N/mm]\"]\n",
    "\n",
    "# Loop through each pair of variables to simulate and find the standard deviation\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        ax = axes[i, j]\n",
    "        if i == j:\n",
    "            # Plot the diagonal terms: mean and uncertainty bounds\n",
    "            X1 = linspace_values[i]\n",
    "            input_grid = np.tile(optimal_params, (len(X1), 1))\n",
    "            input_grid[:, i] = X1\n",
    "\n",
    "            input_grid_normalized = X_scaler.transform(input_grid)\n",
    "            input_grid_normalized = torch.tensor(input_grid_normalized, device=device, dtype=torch.double)\n",
    "            with torch.no_grad():\n",
    "                posterior = gp_model_tscm.posterior(input_grid_normalized)\n",
    "                variance = posterior.variance.cpu().numpy().reshape(X1.shape)  # Use raw variance data\n",
    "\n",
    "                # Calculate standard deviation in the scaled space\n",
    "                std_scaled = np.sqrt(variance)\n",
    "\n",
    "                # Inverse transform the standard deviation to the original scale\n",
    "                std_unscaled = Y_scaler.inverse_transform(std_scaled.reshape(-1, 1)).reshape(X1.shape)\n",
    "\n",
    "            ax.plot(X1, std_unscaled, color='black')\n",
    "            ax.set_xlabel(labels[i], labelpad=5)\n",
    "            ax.set_ylabel('Damage Parameter ($\\psi$)', labelpad=5)\n",
    "        else:\n",
    "            X1, X2 = np.meshgrid(linspace_values[i], linspace_values[j])\n",
    "            input_grid = np.zeros((X1.size, num_vars))\n",
    "            input_grid[:, i] = X1.ravel()\n",
    "            input_grid[:, j] = X2.ravel()\n",
    "            for k in range(num_vars):\n",
    "                if k != i and k != j:\n",
    "                    input_grid[:, k] = optimal_params[k]\n",
    "\n",
    "            input_grid_normalized = X_scaler.transform(input_grid)\n",
    "            input_grid_normalized = torch.tensor(input_grid_normalized, device=device, dtype=torch.double)\n",
    "            with torch.no_grad():\n",
    "                posterior = gp_model_tscm.posterior(input_grid_normalized)\n",
    "                variance = posterior.variance.cpu().numpy().reshape(X1.shape)  # Use raw variance data\n",
    "\n",
    "                # Calculate standard deviation in the scaled space\n",
    "                std_scaled = np.sqrt(variance)\n",
    "\n",
    "                # Inverse transform the standard deviation to the original scale\n",
    "                std_unscaled = Y_scaler.inverse_transform(std_scaled.reshape(-1, 1)).reshape(X1.shape)\n",
    "\n",
    "            # Plot the unscaled standard deviation\n",
    "            cont = ax.contourf(X2, X1, std_unscaled, cmap='coolwarm', extend='both')\n",
    "            ax.set_xlabel(labels[j], labelpad=5)\n",
    "            ax.set_ylabel(labels[i], labelpad=5)\n",
    "\n",
    "# Create the colorbar\n",
    "cbar = fig.colorbar(cont, ax=axes.ravel().tolist(), orientation='vertical', shrink=0.8, aspect=20, extend='both', pad=0.025)\n",
    "# Set the colorbar to display values with two decimal points\n",
    "cbar.ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "cbar.set_label('Damage Parameter ($\\psi$) Standard Deviation \\n **in region close to the optimal solution', rotation=270, labelpad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong> 3.2.2 | Plotting the Kernel, Covariance between variables</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 20\n",
    "colormap = 'coolwarm'\n",
    "figsize = (10, 8)\n",
    "num_vars = len(bounds)\n",
    "labels = [\"Young's Modulus $E$ [MPa]\", \"Tensile Strength $f_{t}$ [MPa]\", \"Mode I, Fracture Energy $G_{f-I}$ [N/mm]\"]\n",
    "gp_model = gp_model_tscm\n",
    "\n",
    "fig, axes = plt.subplots(num_vars, num_vars, figsize=figsize)\n",
    "fig.subplots_adjust(wspace=0.35, hspace=0.3)\n",
    "\n",
    "def compute_covariance(i, j):\n",
    "    X1 = np.linspace(bounds[i][0], bounds[i][1], grid_size)\n",
    "    X2 = np.linspace(bounds[j][0], bounds[j][1], grid_size)\n",
    "\n",
    "    set1 = np.tile(optimal_params, (grid_size, 1))\n",
    "    set2 = np.tile(optimal_params, (grid_size, 1))\n",
    "\n",
    "    set1[:, i] = X1  \n",
    "    set2[:, j] = X2  \n",
    "\n",
    "    # Normalize the arrays\n",
    "    set1_normalized = X_scaler.transform(set1)\n",
    "    set2_normalized = X_scaler.transform(set2)\n",
    "\n",
    "    set1_normalized = torch.tensor(set1_normalized, device=device, dtype=torch.double)\n",
    "    set2_normalized = torch.tensor(set2_normalized, device=device, dtype=torch.double)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cov_matrix = gp_model.covar_module(set1_normalized, set2_normalized).evaluate().cpu().numpy()\n",
    "\n",
    "    cov_matrix = cov_matrix.reshape(grid_size, grid_size)\n",
    "    cov_matrix = Y_scaler.inverse_transform(cov_matrix.reshape(-1, 1)).reshape(grid_size, grid_size)\n",
    "\n",
    "    return X1, X2, cov_matrix\n",
    "\n",
    "all_covariances = []\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        X1, X2, cov_matrix = compute_covariance(i, j)\n",
    "        all_covariances.append(cov_matrix)\n",
    "\n",
    "all_covariances_flat = np.concatenate([cov.flatten() for cov in all_covariances])\n",
    "mean_covariance = np.mean(all_covariances_flat)\n",
    "\n",
    "vmin = np.min(all_covariances_flat - mean_covariance)\n",
    "vmax = np.max(all_covariances_flat - mean_covariance)\n",
    "\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        ax = axes[i, j]\n",
    "        X1, X2, cov_matrix = compute_covariance(i, j)\n",
    "\n",
    "        cov_matrix_deviation = cov_matrix - mean_covariance\n",
    "\n",
    "        cont = ax.contourf(X2, X1, cov_matrix_deviation, cmap=colormap, vmin=vmin, vmax=vmax, levels=10, extend='both')\n",
    "\n",
    "        ax.set_xlabel(labels[j], labelpad=5)\n",
    "        ax.set_ylabel(labels[i], labelpad=5)\n",
    "\n",
    "cbar = fig.colorbar(cont, ax=axes.ravel().tolist(), orientation='vertical', shrink=0.8, aspect=20, extend='both', pad=0.025)\n",
    "\n",
    "cbar.ax.yaxis.set_major_formatter(FormatStrFormatter('%.1e'))\n",
    "cbar.set_label(f'Model parameters covariance ($\\\\mu$ = {mean_covariance:.3f})\\n**in region close to the optimal solution', rotation=270, labelpad=20)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TUD-JF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
